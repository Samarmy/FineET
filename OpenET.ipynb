{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b632bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openet.ssebop as model\n",
    "import ee\n",
    "import pprint\n",
    "from IPython.display import Image\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pvcz\n",
    "import torch\n",
    "import xarray as xr\n",
    "from datetime import date, datetime, timedelta\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import avg_pool2d, interpolate\n",
    "import os\n",
    "from os import path\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import rasterio as rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdbfce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b397dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elevation(lon, lat, units='Meters', output='json'):\n",
    "    \n",
    "    URL = 'https://nationalmap.gov/epqs/pqs.php?'\n",
    "    PARAMS = {'x':str(lon),\n",
    "              'y':str(lat),\n",
    "              'units':units,\n",
    "              'output':output}\n",
    "  \n",
    "    return requests.get(url = URL, params = PARAMS).json()['USGS_Elevation_Point_Query_Service']['Elevation_Query']['Elevation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccde97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_koppen(lon, lat):\n",
    "    clims = ['DFB', 'BWK', 'CFA', 'CWA', 'DWB', 'DFC', 'DFA', 'BSK', 'CSA', 'BSH']\n",
    "    URL = 'http://climateapi.scottpinkelman.com/api/v1/location/' + str(lat) + '/' + str(lon)\n",
    "    return torch.nn.functional.one_hot(torch.tensor(clims.index(requests.get(url = URL).json()[\"return_values\"][0]['koppen_geiger_zone'].upper())), num_classes=len(clims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3c11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255e18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283a38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n",
    "gridmet = ee.ImageCollection(\"IDAHO_EPSCOR/GRIDMET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d0d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_date = '2020-05-01'\n",
    "\n",
    "# Final date of interest (exclusive).\n",
    "f_date = '2020-09-30'\n",
    "\n",
    "# Selection of appropriate bands and dates for LST.\n",
    "lc = lc.filterDate(i_date, f_date)\n",
    "\n",
    "scale = 1000  # scale in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09898a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# site1 = (-110.8661, 31.8214)\n",
    "site1 = (-105.0000, 40.6525)\n",
    "# site1 = (-108.6730, 37.2246)\n",
    "# site1 = (-102.3020, 39.7312)\n",
    "site1point = ee.Geometry.Rectangle(site1[0], site1[1], site1[0], site1[1])\n",
    "# site2point = ee.Geometry.Point(site2[0], site2[1])\n",
    "# site3point = ee.Geometry.Point(site3[0], site3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7605ff12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LC08_033032_20200508',\n",
       " 'LC08_033032_20200524',\n",
       " 'LC08_033032_20200609',\n",
       " 'LC08_033032_20200625',\n",
       " 'LC08_033032_20200711',\n",
       " 'LC08_033032_20200727',\n",
       " 'LC08_033032_20200812',\n",
       " 'LC08_033032_20200828',\n",
       " 'LC08_033032_20200913',\n",
       " 'LC08_033032_20200929',\n",
       " 'LC08_034032_20200515',\n",
       " 'LC08_034032_20200531',\n",
       " 'LC08_034032_20200616',\n",
       " 'LC08_034032_20200702',\n",
       " 'LC08_034032_20200718',\n",
       " 'LC08_034032_20200803',\n",
       " 'LC08_034032_20200819',\n",
       " 'LC08_034032_20200904',\n",
       " 'LC08_034032_20200920']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_r_poi = lc.getRegion(site1point, scale).getInfo()\n",
    "id_list = [x[0] for x in lc_r_poi[1:]]\n",
    "id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd924552",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = 'LC08_033032_20200929'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0322641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020_05_08',\n",
       " '2020_05_15',\n",
       " '2020_05_24',\n",
       " '2020_05_31',\n",
       " '2020_06_09',\n",
       " '2020_06_16',\n",
       " '2020_06_25',\n",
       " '2020_07_02',\n",
       " '2020_07_11',\n",
       " '2020_07_18',\n",
       " '2020_07_27',\n",
       " '2020_08_03',\n",
       " '2020_08_12',\n",
       " '2020_08_19',\n",
       " '2020_08_28',\n",
       " '2020_09_04',\n",
       " '2020_09_13',\n",
       " '2020_09_20',\n",
       " '2020_09_29']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dats = [x[0].split(\"_\")[-1] for x in lc_r_poi[1:]]\n",
    "dats = [x[0].split(\"_\")[-1][:4] + \"_\" + x[0].split(\"_\")[-1][4:6] + \"_\" + x[0].split(\"_\")[-1][6:] for x in lc_r_poi[1:]]\n",
    "dats.sort()\n",
    "dats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde8ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_palette = ['#000000', '#FFFFFF']\n",
    "# ndvi_palette = ['#EFE7E1', '#003300']\n",
    "et_palette = [\n",
    "    'DEC29B', 'E6CDA1', 'EDD9A6', 'F5E4A9', 'FFF4AD', 'C3E683', '6BCC5C', \n",
    "    '3BB369', '20998F', '1C8691', '16678A', '114982', '0B2C7A']\n",
    "viridis_palette = ['440154', '433982', '30678D', '218F8B', '36B677', '8ED542', 'FDE725']\n",
    "\n",
    "\n",
    "image_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c32abace",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = img_id[-8:-4] + \"_\" + img_id[-4:-2] + \"_\" + img_id[-2:]\n",
    "os.makedirs(date, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df63605",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_img = ee.Image('LANDSAT/LC08/C02/T1_L2/' + img_id)\n",
    "landsat_region = site1point.buffer(225).bounds()\n",
    "landsat_region2 = site1point.buffer(3834).bounds()\n",
    "dats1 = [date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "884cfee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_url = landsat_img.select(['SR_B4', 'SR_B3', 'SR_B2'])\\\n",
    "#     .multiply([0.0000275, 0.0000275, 0.0000275])\\\n",
    "#     .add([-0.2, -0.2, -0.2])\\\n",
    "#     .getThumbURL({'min': 0.0, 'max': 0.3, \n",
    "#                   'region': landsat_region, 'dimensions': image_size})\n",
    "# Image(image_url, embed=True, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7057f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SSEBop object from the Landsat image\n",
    "model_obj = model.Image.from_landsat_c2_sr(\n",
    "    landsat_img, \n",
    "    tcorr_source='FANO',\n",
    "    # et_reference_source='projects/climate-engine/cimis/daily', \n",
    "    # et_reference_band='ETr_ASCE',\n",
    "    et_reference_source='IDAHO_EPSCOR/GRIDMET', \n",
    "    et_reference_band='etr', \n",
    "    et_reference_factor=0.85,\n",
    "    et_reference_resample='nearest',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4bb67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_url = model_obj.et\\\n",
    "#     .getThumbURL({'min': 0.0, 'max': 8, 'palette': et_palette, \n",
    "#                   'region': landsat_region2, 'dimensions': image_size})\n",
    "# Image(image_url, embed=True, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21e3207c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 256)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = model_obj.lst.getDownloadUrl({\n",
    "    'region': landsat_region2,\n",
    "    'format': 'GEO_TIFF'\n",
    "})\n",
    "response = requests.get(url)\n",
    "with open(date + '/LST_' + img_id + '.tif', 'wb') as lst_fd:\n",
    "    lst_fd.write(response.content)\n",
    "lst_fd.close()   \n",
    "lst_file = rio.open(date + '/LST_' + img_id + '.tif')\n",
    "lst_file = lst_file.read()\n",
    "lst_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4b6c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = model_obj.ndvi.getDownloadUrl({\n",
    "    'region': landsat_region2,\n",
    "    'format': 'GEO_TIFF'\n",
    "})\n",
    "response = requests.get(url)\n",
    "with open(date + '/NDVI_' + img_id + '.tif', 'wb') as ndvi_fd:\n",
    "    ndvi_fd.write(response.content)\n",
    "ndvi_fd.close()\n",
    "# ndvi_file = rio.open('NDVI_' + img_id + '.tif')\n",
    "# ndvi_file = ndvi_file.read()\n",
    "# ndvi_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc3544e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = model_obj.et.getDownloadUrl({\n",
    "    'region': landsat_region2,\n",
    "    'format': 'GEO_TIFF'\n",
    "})\n",
    "response = requests.get(url)\n",
    "with open(date + '/OPENET_' + img_id + '.tif', 'wb') as openet_fd:\n",
    "    openet_fd.write(response.content)\n",
    "openet_fd.close()\n",
    "# openet_file = rio.open('OPENET_' + img_id + '.tif')\n",
    "# openet_file = openet_file.read()\n",
    "# openet_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b5f8d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_url = model_obj.ndvi\\\n",
    "#     .getThumbURL({'min': 0.0, 'max': 1, 'palette': ndvi_palette, \n",
    "#                   'region': landsat_region, 'dimensions': image_size})\n",
    "# Image(image_url, embed=True, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f66cf608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_obj.et.reduceRegion(ee.Reducer.count(), landsat_region).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60fe7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.et.reduceRegion(ee.Reducer.max(), landsat_region).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61014a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.et.reduceRegion(ee.Reducer.mean(), landsat_region).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c5edb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.ndvi.reduceRegion(ee.Reducer.max(), landsat_region).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7408080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.ndvi.reduceRegion(ee.Reducer.mean(), landsat_region).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1488110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_obj.lst.reduceRegion(ee.Reducer.max(), landsat_region).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d359cd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_obj.lst.reduceRegion(ee.Reducer.mean(), landsat_region).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efb9bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# landsat_img.sampleRectangle(landsat_region).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c281e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# landsat_img.sampleRectangle(landsat_region).getInfo()[\"properties\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "500e4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = (np.expand_dims(np.array(landsat_img.sampleRectangle(landsat_region).getInfo()[\"properties\"][\"SR_B2\"]), axis=0).astype(float)*0.0000275)-0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd6b3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "green = (np.expand_dims(np.array(landsat_img.sampleRectangle(landsat_region).getInfo()[\"properties\"][\"SR_B3\"]), axis=0).astype(float)*0.0000275)-0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bab08044",
   "metadata": {},
   "outputs": [],
   "source": [
    "red = (np.expand_dims(np.array(landsat_img.sampleRectangle(landsat_region).getInfo()[\"properties\"][\"SR_B4\"]), axis=0).astype(float)*0.0000275)-0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f41e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nir = (np.expand_dims(np.array(landsat_img.sampleRectangle(landsat_region).getInfo()[\"properties\"][\"SR_B5\"]), axis=0).astype(float)*0.0000275)-0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ab9c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "swir1 = (np.expand_dims(np.array(landsat_img.sampleRectangle(landsat_region).getInfo()[\"properties\"][\"SR_B6\"]), axis=0).astype(float)*0.0000275)-0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5260facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = (((np.expand_dims(np.array(landsat_img.sampleRectangle(landsat_region).getInfo()[\"properties\"][\"ST_B10\"]), axis=0).astype(float)*0.00341802) + 149.0)/400.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2635b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = np.expand_dims(np.array(landsat_img.sampleRectangle(landsat_region).getInfo()[\"properties\"][\"QA_PIXEL\"]), axis=0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1016530",
   "metadata": {},
   "outputs": [],
   "source": [
    "lons = np.array([x[0] for x in landsat_img.sampleRectangle(landsat_region).getInfo()[\"geometry\"][\"coordinates\"][0]])\n",
    "lons = np.expand_dims(np.tile(np.expand_dims(np.linspace(lons.min(), lons.max(), 16), axis=0), (16, 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d911b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.array([x[1] for x in landsat_img.sampleRectangle(landsat_region).getInfo()[\"geometry\"][\"coordinates\"][0]])\n",
    "lats = np.expand_dims(np.tile(np.expand_dims(np.linspace(lats.min(), lats.max(), 16), axis=-1), (1, 16)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c28a80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_seq = torch.tensor(np.concatenate([blue, green, red, nir, swir1, lst, qa, lons, lats])).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "726d2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, lon, lat, clim = dats1, torch.tensor([site1[0]]), torch.tensor([site1[1]]), get_koppen(site1[0], site1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47262f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_obj = datetime.strptime(dats1[0].split(\"_\")[0] + '_' + dats1[0].split(\"_\")[1] + '_' + dats1[0].split(\"_\")[2], '%Y_%m_%d')\n",
    "day_of_year = date_time_obj.timetuple().tm_yday\n",
    "day_sin = torch.tensor([np.sin(2 * np.pi * day_of_year/364.0)])\n",
    "day_cos = torch.tensor([np.cos(2 * np.pi * day_of_year/364.0)])\n",
    "x_coord = torch.tensor([np.sin(math.pi/2-np.deg2rad(site1[1])) * np.cos(np.deg2rad(site1[0]))])\n",
    "y_coord = torch.tensor([np.sin(math.pi/2-np.deg2rad(site1[1])) * np.sin(np.deg2rad(site1[0]))])\n",
    "z_coord = torch.tensor([np.cos(math.pi/2-np.deg2rad(site1[1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d64b1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmin = np.expand_dims(np.tile(np.array(gridmet_img.sampleRectangle(gridmet_region).getInfo()[\"properties\"]['tmmn']), [16,16]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05402d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmax = np.expand_dims(np.tile(np.array(gridmet_img.sampleRectangle(gridmet_region).getInfo()[\"properties\"]['tmmx']), [16,16]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a095bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# srad = np.expand_dims(np.tile(np.array(gridmet_img.sampleRectangle(gridmet_region).getInfo()[\"properties\"]['srad']), [16,16]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d375b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sph = np.expand_dims(np.tile(np.array(gridmet_img.sampleRectangle(gridmet_region).getInfo()[\"properties\"]['sph']), [16,16]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3023613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# etr = np.expand_dims(np.tile(np.array(gridmet_img.sampleRectangle(gridmet_region).getInfo()[\"properties\"]['etr']), [16,16]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2a616e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev = torch.tensor([get_elevation(site1[0], site1[1])/8848.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "deb74a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSEBop(torch.nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()    \n",
    "        self.lc = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n",
    "        self.scale = 1000\n",
    "        self.device = device\n",
    "        self.dict = {}\n",
    "        \n",
    "    \n",
    "    def _openet(self, date, lon, lat):\n",
    "        date_1 = [datetime.strptime(d1, \"%Y-%m-%d\") for d1 in date]\n",
    "        date_next_day = [str(d2 + timedelta(days=1))[:10] for d2 in date_1]\n",
    "        ets = []\n",
    "        \n",
    "        for i1 in range(len(date)):\n",
    "            \n",
    "            if str(date[i1]) + \"_\" + str(lon[i1]) + \"_\" + str(lat[i1]) in self.dict:\n",
    "                ets.append(self.dict[str(date[i1]) + \"_\" + str(lon[i1]) + \"_\" + str(lat[i1])])\n",
    "            else:\n",
    "                lc = self.lc.filterDate(date[i1], date_next_day[i1])\n",
    "                site1point = ee.Geometry.Rectangle(lon[i1].item(), lat[i1].item(), lon[i1].item(), lat[i1].item())\n",
    "                lc_r_poi = lc.getRegion(site1point, self.scale).getInfo()\n",
    "                id_list = [x[0] for x in lc_r_poi[1:]]\n",
    "                id_list.sort()\n",
    "                landsat_img = ee.Image('LANDSAT/LC08/C02/T1_L2/' + id_list[-1])\n",
    "                landsat_region = site1point.buffer(225)\n",
    "                model_obj = model.Image.from_landsat_c2_sr(\n",
    "                    landsat_img, \n",
    "                    tcorr_source='FANO',\n",
    "                    et_reference_source='IDAHO_EPSCOR/GRIDMET', \n",
    "                    et_reference_band='etr', \n",
    "                    et_reference_factor=0.85,\n",
    "                    et_reference_resample='nearest',\n",
    "                )\n",
    "                et = model_obj.et.reduceRegion(ee.Reducer.max(), landsat_region).getInfo()[\"et\"]\n",
    "                if et:\n",
    "                    ets.append(et)\n",
    "                    self.dict[str(date[i1]) + \"_\" + str(lon[i1]) + \"_\" + str(lat[i1])] = et\n",
    "                else:\n",
    "                    ets.append(0.0)\n",
    "                    self.dict[str(date[i1]) + \"_\" + str(lon[i1]) + \"_\" + str(lat[i1])] = 0.0\n",
    "        \n",
    "        return ets\n",
    "    \n",
    "    def forward(self, date, lat, lon):\n",
    "        #nir red lst\n",
    "        # etr, elev, sph, srad, tmin, tmax, lat, doy\n",
    "        \n",
    "#         data = data\n",
    "        date_str = [dstr.replace(\"_\", \"-\") for dstr in date]\n",
    "        \n",
    "        return torch.tensor(self._openet(date_str, lon, lat), device=self.device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c611da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssebop_model = SSEBop(device).to(device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6941346",
   "metadata": {},
   "outputs": [],
   "source": [
    "lons1 = torch.tensor([[site1[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d2a53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats1 = torch.tensor([[site1[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3be1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ET = ssebop_model(dats1, lats1.to(device, dtype=torch.float32), lons1.to(device, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23255f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, stride = 1):\n",
    "    return nn.Conv2d(in_channels,out_channels,kernel_size = 1,\n",
    "                    stride =stride, padding=0,bias=False)\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride = 1):\n",
    "    return nn.Conv2d(in_channels,out_channels,kernel_size = 3,\n",
    "        stride =stride, padding=1,bias=False)\n",
    "\n",
    "class irnn_layer(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(irnn_layer,self).__init__()\n",
    "        self.left_weight = nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,groups=in_channels,padding=0)\n",
    "        self.right_weight = nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,groups=in_channels,padding=0)\n",
    "        self.up_weight = nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,groups=in_channels,padding=0)\n",
    "        self.down_weight = nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,groups=in_channels,padding=0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        _,_,H,W = x.shape\n",
    "        top_left = x.clone()\n",
    "        top_right = x.clone()\n",
    "        top_up = x.clone()\n",
    "        top_down = x.clone()\n",
    "        top_left[:,:,:,1:] = F.relu(self.left_weight(x)[:,:,:,:W-1]+x[:,:,:,1:],inplace=False)\n",
    "        top_right[:,:,:,:-1] = F.relu(self.right_weight(x)[:,:,:,1:]+x[:,:,:,:W-1],inplace=False)\n",
    "        top_up[:,:,1:,:] = F.relu(self.up_weight(x)[:,:,:H-1,:]+x[:,:,1:,:],inplace=False)\n",
    "        top_down[:,:,:-1,:] = F.relu(self.down_weight(x)[:,:,1:,:]+x[:,:,:H-1,:],inplace=False)\n",
    "        return (top_up,top_right,top_down,top_left)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(Attention,self).__init__()\n",
    "        self.out_channels = int(in_channels/2)\n",
    "        self.conv1 = nn.Conv2d(in_channels,self.out_channels,kernel_size=3,padding=1,stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(self.out_channels,self.out_channels,kernel_size=3,padding=1,stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(self.out_channels,4,kernel_size=1,padding=0,stride=1)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.sigmod(out)\n",
    "        return out\n",
    "\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,attention=1):\n",
    "        super(SAM,self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.irnn1 = irnn_layer(self.out_channels)\n",
    "        self.irnn2 = irnn_layer(self.out_channels)\n",
    "        self.conv_in = conv3x3(in_channels,self.out_channels)\n",
    "        self.relu1 = nn.ReLU(True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.out_channels,self.out_channels,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2 = nn.Conv2d(self.out_channels*4,self.out_channels,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv3 = nn.Conv2d(self.out_channels*4,self.out_channels,kernel_size=1,stride=1,padding=0)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "        self.attention = attention\n",
    "        if self.attention:\n",
    "            self.attention_layer = Attention(in_channels)\n",
    "        self.conv_out = conv1x1(self.out_channels,1)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.attention:\n",
    "            weight = self.attention_layer(x)\n",
    "        out = self.conv1(x)\n",
    "        top_up,top_right,top_down,top_left = self.irnn1(out)\n",
    "        \n",
    "        # direction attention\n",
    "        if self.attention:\n",
    "            top_up.mul(weight[:,0:1,:,:])\n",
    "            top_right.mul(weight[:,1:2,:,:])\n",
    "            top_down.mul(weight[:,2:3,:,:])\n",
    "            top_left.mul(weight[:,3:4,:,:])\n",
    "        out = torch.cat([top_up,top_right,top_down,top_left],dim=1)\n",
    "        out = self.conv2(out)\n",
    "        top_up,top_right,top_down,top_left = self.irnn2(out)\n",
    "        \n",
    "        # direction attention\n",
    "        if self.attention:\n",
    "            top_up.mul(weight[:,0:1,:,:])\n",
    "            top_right.mul(weight[:,1:2,:,:])\n",
    "            top_down.mul(weight[:,2:3,:,:])\n",
    "            top_left.mul(weight[:,3:4,:,:])\n",
    "        \n",
    "        out = torch.cat([top_up,top_right,top_down,top_left],dim=1)\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu2(out)\n",
    "        mask = self.sigmod(self.conv_out(out))\n",
    "        return mask\n",
    "\n",
    "class convolutionalCapsule(nn.Module):\n",
    "    def __init__(self, in_capsules, out_capsules, in_channels, out_channels, stride=1, padding=2,\n",
    "                 kernel=5, num_routes=3, nonlinearity='sqaush', batch_norm=False, dynamic_routing='local', cuda=False):\n",
    "        super(convolutionalCapsule, self).__init__()\n",
    "        self.num_routes = num_routes\n",
    "        self.in_channels = in_channels\n",
    "        self.in_capsules = in_capsules\n",
    "        self.out_capsules = out_capsules\n",
    "        self.out_channels = out_channels\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = nn.BatchNorm2d(in_capsules*out_capsules*out_channels)\n",
    "        self.conv2d = nn.Conv2d(kernel_size=(kernel, kernel), stride=stride, padding=padding,\n",
    "                                in_channels=in_channels, out_channels=out_channels*out_capsules)\n",
    "        self.dynamic_routing = dynamic_routing\n",
    "        self.cuda = cuda\n",
    "        self.SAM1 = SAM(self.in_channels,self.in_channels,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        in_width, in_height = x.size(3), x.size(4)\n",
    "        x = x.view(batch_size*self.in_capsules, self.in_channels, in_width, in_height)\n",
    "        u_hat = self.conv2d(x) * self.SAM1(x)\n",
    "\n",
    "        out_width, out_height = u_hat.size(2), u_hat.size(3)\n",
    "\n",
    "        # batch norm layer\n",
    "        if self.batch_norm:\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules, self.out_capsules * self.out_channels, out_width, out_height)\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules * self.out_capsules * self.out_channels, out_width, out_height)\n",
    "            u_hat = self.bn(u_hat)\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules, self.out_capsules*self.out_channels, out_width, out_height)\n",
    "            u_hat = u_hat.permute(0,1,3,4,2).contiguous()\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules, out_width, out_height, self.out_capsules, self.out_channels)\n",
    "\n",
    "        else:\n",
    "            u_hat = u_hat.permute(0,2,3,1).contiguous()\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules, out_width, out_height, self.out_capsules*self.out_channels)\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules, out_width, out_height, self.out_capsules, self.out_channels)\n",
    "\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.in_capsules, out_width, out_height, self.out_capsules))\n",
    "        if self.cuda:\n",
    "            b_ij = b_ij.cuda()\n",
    "        for iteration in range(self.num_routes):\n",
    "            c_ij = F.softmax(b_ij, dim=1)\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(5)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "            if (self.nonlinearity == 'relu') and (iteration == self.num_routes - 1):\n",
    "                v_j = F.relu(s_j)\n",
    "            elif (self.nonlinearity == 'leakyRelu') and (iteration == self.num_routes - 1):\n",
    "                v_j = F.leaky_relu(s_j)\n",
    "            else:\n",
    "                v_j = self.squash(s_j)\n",
    "\n",
    "            v_j = v_j.squeeze(1)\n",
    "\n",
    "            if iteration < self.num_routes - 1:\n",
    "                temp = u_hat.permute(0, 2, 3, 4, 1, 5)\n",
    "                temp2 = v_j.unsqueeze(5)\n",
    "                a_ij = torch.matmul(temp, temp2).squeeze(5) # dot product here\n",
    "                a_ij = a_ij.permute(0, 4, 1, 2, 3)\n",
    "                b_ij = b_ij + a_ij.mean(dim=0)\n",
    "\n",
    "        v_j = v_j.permute(0, 3, 4, 1, 2).contiguous()\n",
    "\n",
    "        return v_j\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_caps, num_input_features, growth_rate, bn_size, drop_rate, actvec_size):\n",
    "        super().__init__()\n",
    "        self.concap1 = convolutionalCapsule(in_capsules=8, out_capsules=8, in_channels=8,\n",
    "                                  out_channels=8,\n",
    "                                  stride=1, padding=1, kernel=3, num_routes=3,\n",
    "                                  nonlinearity='sqaush', batch_norm=True,\n",
    "                                  dynamic_routing='local', cuda=True)\n",
    "        \n",
    "        self.concap2 = convolutionalCapsule(in_capsules=8, out_capsules=8, in_channels=8,\n",
    "                                  out_channels=8,\n",
    "                                  stride=1, padding=1, kernel=3, num_routes=3,\n",
    "                                  nonlinearity='sqaush', batch_norm=True,\n",
    "                                  dynamic_routing='local', cuda=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = self.concap1(x)\n",
    "        new_features = self.concap2(new_features)\n",
    "        return x + new_features\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_layers, num_caps, num_input_features, bn_size, growth_rate,\n",
    "                 drop_rate, actvec_size):\n",
    "        super().__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_caps, num_input_features,\n",
    "                                growth_rate, bn_size, drop_rate, actvec_size)\n",
    "            self.add_module('denselayer{}'.format(i + 1), layer)\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_caps, in_vect, out_vect):\n",
    "        super().__init__()\n",
    "        self.skip = convolutionalCapsule(in_capsules=8, out_capsules=8,\n",
    "                                     in_channels=8, out_channels=8,\n",
    "                                  stride=1, kernel=1, padding=0, num_routes=3,\n",
    "                                  nonlinearity='sqaush', batch_norm=True,\n",
    "                                  dynamic_routing='local', cuda=True)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.skip(x)\n",
    "        batch_size, num_caps = out.size(0), out.size(1)\n",
    "        out = out.view(out.shape[0] * out.shape[1], out.shape[2], out.shape[3], out.shape[4])\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(batch_size, num_caps, out.shape[1], int(out.shape[2]), int(out.shape[3]))\n",
    "        return out\n",
    "\n",
    "class DenseNetModel(nn.Module):\n",
    "    \"\"\"Densenet-BC model class\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_input_channels=5,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 growth_rate=32,\n",
    "                 block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64,\n",
    "                 bn_size=32,\n",
    "                 drop_rate=0,\n",
    "                 num_classes=1,\n",
    "                 actvec_size=8):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_init_features = num_init_features\n",
    "        self.actvec_size = actvec_size\n",
    "        self.num_caps = int(num_init_features/actvec_size)\n",
    "\n",
    "        # First convolution\n",
    "        self.input_features = [('conv1',\n",
    "                          nn.Conv2d(n_input_channels,\n",
    "                                    num_init_features,\n",
    "                                    kernel_size=4,\n",
    "                                    stride=2,\n",
    "                                    padding=1,\n",
    "                                    bias=False)),\n",
    "                         ('norm1', nn.BatchNorm2d(num_init_features)),\n",
    "                         ('relu1', nn.ReLU(inplace=True))]\n",
    "        if not no_max_pool:\n",
    "            self.input_features.append(\n",
    "                ('pool1', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)))\n",
    "        self.input_features = nn.Sequential(OrderedDict(self.input_features))\n",
    "        self.features = nn.Sequential()\n",
    "        # Each denseblock\n",
    "        num_actvec = actvec_size\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers,\n",
    "                                num_caps=self.num_caps,\n",
    "                                num_input_features=num_actvec,\n",
    "                                bn_size=bn_size,\n",
    "                                growth_rate=growth_rate,\n",
    "                                drop_rate=drop_rate,\n",
    "                                actvec_size=actvec_size)\n",
    "            self.features.add_module('denseblock{}'.format(i + 1), block)\n",
    "            num_actvec = num_actvec + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_caps = self.num_caps, \n",
    "                                    in_vect=actvec_size, \n",
    "                                    out_vect=actvec_size)\n",
    "                self.features.add_module('transition{}'.format(i + 1), trans)\n",
    "                num_actvec = num_actvec // 2\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "        self.metadata_network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(17, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(64, 128)\n",
    "        )\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(64 + 128, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, metadata):\n",
    "        input_features = self.input_features(x)\n",
    "        input_features = input_features.view(input_features.shape[0], int(self.num_init_features/self.actvec_size), self.actvec_size, input_features.shape[-2], input_features.shape[-1])\n",
    "\n",
    "        \n",
    "        \n",
    "        out = self.features(input_features)\n",
    "        y = self.metadata_network(metadata)\n",
    "        out = self.classifier(torch.cat((out.view(out.size(0), -1), y), dim=1))\n",
    "        return out\n",
    "    \n",
    "#     def forward(self, x, metadata):\n",
    "#         out = self.convolutions(x)\n",
    "#         out = out.view(x.size(0), -1)\n",
    "#         out_metadata = self.metadata_network(metadata)\n",
    "#         out = metadata[:, 0] + self.fc(torch.cat((out, out_metadata), dim=1))\n",
    "\n",
    "# #         out = ((torch.sigmoid(self.fc(torch.cat((out, out_metadata), dim=1))) * (self.range2 - self.range1)) + self.range1)\n",
    "#         return out\n",
    "\n",
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [121, 169, 201, 264]\n",
    "\n",
    "    if model_depth == 121:\n",
    "        model = DenseNetModel(num_init_features=64,\n",
    "                         growth_rate=4,\n",
    "                         block_config=(6, 12, 24, 16),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 169:\n",
    "        model = DenseNetModel(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 32, 32),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 201:\n",
    "        model = DenseNetModel(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 48, 32),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 264:\n",
    "        model = DenseNetModel(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 64, 48),\n",
    "                         **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.SubModel = generate_model(121)\n",
    "        \n",
    "#     seq_len, batch, input_size\n",
    "    def forward(self, x, y):\n",
    "        out = self.SubModel(x, y)\n",
    "        return out.flatten()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "099e66e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5094], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor([5.9784], device='cuda:0')\n",
      "tensor([4.4689], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'lst': 308.84028728}\n",
      "{'ndvi': 0.8939027184253256}\n"
     ]
    }
   ],
   "source": [
    "quench_model = torch.load(\"../Checkpoints/Quench_50.pt\").to(device, dtype=torch.float32)\n",
    "quench_model.eval()\n",
    "output = quench_model(interpolate(img_seq[:, 0:5].to(device, dtype=torch.float32), size=32), torch.cat((output_ET.to(device, dtype=torch.float32), clim.to(device, dtype=torch.float32), day_sin.to(device, dtype=torch.float32), day_cos.to(device, dtype=torch.float32), x_coord.to(device, dtype=torch.float32), y_coord.to(device, dtype=torch.float32), z_coord.to(device, dtype=torch.float32), elev.to(device, dtype=torch.float32))).unsqueeze(0))\n",
    "print(output)\n",
    "print(output_ET)\n",
    "print(output + output_ET)\n",
    "print(model_obj.lst.reduceRegion(ee.Reducer.max(), landsat_region).getInfo())\n",
    "print(model_obj.ndvi.reduceRegion(ee.Reducer.max(), landsat_region).getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d747ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "openet_data = rio.open(date + '/OPENET_' + img_id + '.tif')\n",
    "openet_values = openet_data.read(1)\n",
    "openet_values[openet_values!=0.0] = (openet_values[openet_values!=0.0] + output.item())\n",
    "\n",
    "\n",
    "openet_profile = openet_data.profile\n",
    "with rio.open(date + '/ET_' + img_id + '.tif', 'w', **openet_profile) as et_dst:\n",
    "    et_dst.write(openet_values, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aa286cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quench_model = torch.load(\"../Checkpoints/Quench_250.pt\").to(device, dtype=torch.float32)\n",
    "# quench_model.eval()\n",
    "# output = quench_model(interpolate(img_seq[:, 0:5].to(device, dtype=torch.float32), size=32), torch.cat((output_ET.to(device, dtype=torch.float32), clim.to(device, dtype=torch.float32), day_sin.to(device, dtype=torch.float32), day_cos.to(device, dtype=torch.float32), x_coord.to(device, dtype=torch.float32), y_coord.to(device, dtype=torch.float32), z_coord.to(device, dtype=torch.float32), elev.to(device, dtype=torch.float32))).unsqueeze(0))\n",
    "# print(output)\n",
    "# print(output_ET)\n",
    "# print(model_obj.lst.reduceRegion(ee.Reducer.max(), landsat_region).getInfo())\n",
    "# print(model_obj.ndvi.reduceRegion(ee.Reducer.max(), landsat_region).getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5ada5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quench_model = torch.load(\"../Checkpoints/Quench_500.pt\").to(device, dtype=torch.float32)\n",
    "# quench_model.eval()\n",
    "# output = quench_model(interpolate(img_seq[:, 0:5].to(device, dtype=torch.float32), size=32), torch.cat((output_ET.to(device, dtype=torch.float32), clim.to(device, dtype=torch.float32), day_sin.to(device, dtype=torch.float32), day_cos.to(device, dtype=torch.float32), x_coord.to(device, dtype=torch.float32), y_coord.to(device, dtype=torch.float32), z_coord.to(device, dtype=torch.float32), elev.to(device, dtype=torch.float32))).unsqueeze(0))\n",
    "# print(output)\n",
    "# print(output_ET)\n",
    "# print(model_obj.lst.reduceRegion(ee.Reducer.max(), landsat_region).getInfo())\n",
    "# print(model_obj.ndvi.reduceRegion(ee.Reducer.max(), landsat_region).getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41780f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f26fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd3954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
