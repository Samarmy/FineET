{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# import pickle\n",
    "# import os\n",
    "# import numpy as np\n",
    "# a_file = open(\"Train_Data.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# a_file.close()\n",
    "# os.makedirs(\"Train_Dataset\", exist_ok=True)\n",
    "# train_data = glob(\"Train_Data/*\")\n",
    "# train_dataset = []\n",
    "# for td in train_data:\n",
    "#     if td in output:\n",
    "#         filename = td.replace(\"Train_Data/\", \"Train_Dataset/\" + str(output[td]) + \"_\")\n",
    "    \n",
    "#     data = np.load(td)\n",
    "#     np.save(filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb19b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import avg_pool2d, interpolate\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import math\n",
    "from functools import partial\n",
    "from torchvision import transforms, utils\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "try:\n",
    "    from torch.hub import load_state_dict_from_url\n",
    "except ImportError:\n",
    "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "    \n",
    "from collections import OrderedDict\n",
    "import socket\n",
    "import time\n",
    "import random\n",
    "import rasterio as rio\n",
    "import pvcz\n",
    "from datetime import date, datetime, timedelta\n",
    "import openet.ssebop as model\n",
    "import ee\n",
    "\n",
    "\n",
    "from os import path\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available(): device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34345542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a88f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ab315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, stride = 1):\n",
    "    return nn.Conv2d(in_channels,out_channels,kernel_size = 1,\n",
    "                    stride =stride, padding=0,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride = 1):\n",
    "    return nn.Conv2d(in_channels,out_channels,kernel_size = 3,\n",
    "        stride =stride, padding=1,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb2a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class irnn_layer(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(irnn_layer,self).__init__()\n",
    "        self.left_weight = nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,groups=in_channels,padding=0)\n",
    "        self.right_weight = nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,groups=in_channels,padding=0)\n",
    "        self.up_weight = nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,groups=in_channels,padding=0)\n",
    "        self.down_weight = nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,groups=in_channels,padding=0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        _,_,H,W = x.shape\n",
    "        top_left = x.clone()\n",
    "        top_right = x.clone()\n",
    "        top_up = x.clone()\n",
    "        top_down = x.clone()\n",
    "        top_left[:,:,:,1:] = F.relu(self.left_weight(x)[:,:,:,:W-1]+x[:,:,:,1:],inplace=False)\n",
    "        top_right[:,:,:,:-1] = F.relu(self.right_weight(x)[:,:,:,1:]+x[:,:,:,:W-1],inplace=False)\n",
    "        top_up[:,:,1:,:] = F.relu(self.up_weight(x)[:,:,:H-1,:]+x[:,:,1:,:],inplace=False)\n",
    "        top_down[:,:,:-1,:] = F.relu(self.down_weight(x)[:,:,1:,:]+x[:,:,:H-1,:],inplace=False)\n",
    "        return (top_up,top_right,top_down,top_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a52d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(Attention,self).__init__()\n",
    "        self.out_channels = int(in_channels/2)\n",
    "        self.conv1 = nn.Conv2d(in_channels,self.out_channels,kernel_size=3,padding=1,stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(self.out_channels,self.out_channels,kernel_size=3,padding=1,stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(self.out_channels,4,kernel_size=1,padding=0,stride=1)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.sigmod(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,attention=1):\n",
    "        super(SAM,self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.irnn1 = irnn_layer(self.out_channels)\n",
    "        self.irnn2 = irnn_layer(self.out_channels)\n",
    "        self.conv_in = conv3x3(in_channels,self.out_channels)\n",
    "        self.relu1 = nn.ReLU(True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.out_channels,self.out_channels,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv2 = nn.Conv2d(self.out_channels*4,self.out_channels,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv3 = nn.Conv2d(self.out_channels*4,self.out_channels,kernel_size=1,stride=1,padding=0)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "        self.attention = attention\n",
    "        if self.attention:\n",
    "            self.attention_layer = Attention(in_channels)\n",
    "        self.conv_out = conv1x1(self.out_channels,1)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.attention:\n",
    "            weight = self.attention_layer(x)\n",
    "        out = self.conv1(x)\n",
    "        top_up,top_right,top_down,top_left = self.irnn1(out)\n",
    "        \n",
    "        # direction attention\n",
    "        if self.attention:\n",
    "            top_up.mul(weight[:,0:1,:,:])\n",
    "            top_right.mul(weight[:,1:2,:,:])\n",
    "            top_down.mul(weight[:,2:3,:,:])\n",
    "            top_left.mul(weight[:,3:4,:,:])\n",
    "        out = torch.cat([top_up,top_right,top_down,top_left],dim=1)\n",
    "        out = self.conv2(out)\n",
    "        top_up,top_right,top_down,top_left = self.irnn2(out)\n",
    "        \n",
    "        # direction attention\n",
    "        if self.attention:\n",
    "            top_up.mul(weight[:,0:1,:,:])\n",
    "            top_right.mul(weight[:,1:2,:,:])\n",
    "            top_down.mul(weight[:,2:3,:,:])\n",
    "            top_left.mul(weight[:,3:4,:,:])\n",
    "        \n",
    "        out = torch.cat([top_up,top_right,top_down,top_left],dim=1)\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu2(out)\n",
    "        mask = self.sigmod(self.conv_out(out))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df46dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolutionalCapsule(nn.Module):\n",
    "    def __init__(self, in_capsules, out_capsules, in_channels, out_channels, stride=1, padding=2,\n",
    "                 kernel=5, num_routes=3, nonlinearity='sqaush', batch_norm=False, dynamic_routing='local', cuda=False):\n",
    "        super(convolutionalCapsule, self).__init__()\n",
    "        self.num_routes = num_routes\n",
    "        self.in_channels = in_channels\n",
    "        self.in_capsules = in_capsules\n",
    "        self.out_capsules = out_capsules\n",
    "        self.out_channels = out_channels\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = nn.BatchNorm2d(in_capsules*out_capsules*out_channels)\n",
    "        self.conv2d = nn.Conv2d(kernel_size=(kernel, kernel), stride=stride, padding=padding,\n",
    "                                in_channels=in_channels, out_channels=out_channels*out_capsules)\n",
    "        self.dynamic_routing = dynamic_routing\n",
    "        self.cuda = cuda\n",
    "        self.SAM1 = SAM(self.in_channels,self.in_channels,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        in_width, in_height = x.size(3), x.size(4)\n",
    "        x = x.view(batch_size*self.in_capsules, self.in_channels, in_width, in_height)\n",
    "        u_hat = self.conv2d(x) * self.SAM1(x)\n",
    "\n",
    "        out_width, out_height = u_hat.size(2), u_hat.size(3)\n",
    "\n",
    "        # batch norm layer\n",
    "        if self.batch_norm:\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules, self.out_capsules * self.out_channels, out_width, out_height)\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules * self.out_capsules * self.out_channels, out_width, out_height)\n",
    "            u_hat = self.bn(u_hat)\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules, self.out_capsules*self.out_channels, out_width, out_height)\n",
    "            u_hat = u_hat.permute(0,1,3,4,2).contiguous()\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules, out_width, out_height, self.out_capsules, self.out_channels)\n",
    "\n",
    "        else:\n",
    "            u_hat = u_hat.permute(0,2,3,1).contiguous()\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules, out_width, out_height, self.out_capsules*self.out_channels)\n",
    "            u_hat = u_hat.view(batch_size, self.in_capsules, out_width, out_height, self.out_capsules, self.out_channels)\n",
    "\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.in_capsules, out_width, out_height, self.out_capsules))\n",
    "        if self.cuda:\n",
    "            b_ij = b_ij.cuda()\n",
    "        for iteration in range(self.num_routes):\n",
    "            c_ij = F.softmax(b_ij, dim=1)\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(5)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "            if (self.nonlinearity == 'relu') and (iteration == self.num_routes - 1):\n",
    "                v_j = F.relu(s_j)\n",
    "            elif (self.nonlinearity == 'leakyRelu') and (iteration == self.num_routes - 1):\n",
    "                v_j = F.leaky_relu(s_j)\n",
    "            else:\n",
    "                v_j = self.squash(s_j)\n",
    "\n",
    "            v_j = v_j.squeeze(1)\n",
    "\n",
    "            if iteration < self.num_routes - 1:\n",
    "                temp = u_hat.permute(0, 2, 3, 4, 1, 5)\n",
    "                temp2 = v_j.unsqueeze(5)\n",
    "                a_ij = torch.matmul(temp, temp2).squeeze(5) # dot product here\n",
    "                a_ij = a_ij.permute(0, 4, 1, 2, 3)\n",
    "                b_ij = b_ij + a_ij.mean(dim=0)\n",
    "\n",
    "        v_j = v_j.permute(0, 3, 4, 1, 2).contiguous()\n",
    "\n",
    "        return v_j\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseLayer(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_caps, num_input_features, growth_rate, bn_size, drop_rate, actvec_size):\n",
    "        super().__init__()\n",
    "        self.concap1 = convolutionalCapsule(in_capsules=8, out_capsules=8, in_channels=8,\n",
    "                                  out_channels=8,\n",
    "                                  stride=1, padding=1, kernel=3, num_routes=3,\n",
    "                                  nonlinearity='sqaush', batch_norm=True,\n",
    "                                  dynamic_routing='local', cuda=True)\n",
    "        \n",
    "        self.concap2 = convolutionalCapsule(in_capsules=8, out_capsules=8, in_channels=8,\n",
    "                                  out_channels=8,\n",
    "                                  stride=1, padding=1, kernel=3, num_routes=3,\n",
    "                                  nonlinearity='sqaush', batch_norm=True,\n",
    "                                  dynamic_routing='local', cuda=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = self.concap1(x)\n",
    "        new_features = self.concap2(new_features)\n",
    "        return x + new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc81645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseBlock(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_layers, num_caps, num_input_features, bn_size, growth_rate,\n",
    "                 drop_rate, actvec_size):\n",
    "        super().__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_caps, num_input_features,\n",
    "                                growth_rate, bn_size, drop_rate, actvec_size)\n",
    "            self.add_module('denselayer{}'.format(i + 1), layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bc6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Transition(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_caps, in_vect, out_vect):\n",
    "        super().__init__()\n",
    "        self.skip = convolutionalCapsule(in_capsules=8, out_capsules=8,\n",
    "                                     in_channels=8, out_channels=8,\n",
    "                                  stride=1, kernel=1, padding=0, num_routes=3,\n",
    "                                  nonlinearity='sqaush', batch_norm=True,\n",
    "                                  dynamic_routing='local', cuda=True)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.skip(x)\n",
    "        batch_size, num_caps = out.size(0), out.size(1)\n",
    "        out = out.view(out.shape[0] * out.shape[1], out.shape[2], out.shape[3], out.shape[4])\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(batch_size, num_caps, out.shape[1], int(out.shape[2]), int(out.shape[3]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetModel(nn.Module):\n",
    "    \"\"\"Densenet-BC model class\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_input_channels=5,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 growth_rate=32,\n",
    "                 block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64,\n",
    "                 bn_size=32,\n",
    "                 drop_rate=0,\n",
    "                 num_classes=1,\n",
    "                 actvec_size=8):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_init_features = num_init_features\n",
    "        self.actvec_size = actvec_size\n",
    "        self.num_caps = int(num_init_features/actvec_size)\n",
    "\n",
    "        # First convolution\n",
    "        self.input_features = [('conv1',\n",
    "                          nn.Conv2d(n_input_channels,\n",
    "                                    num_init_features,\n",
    "                                    kernel_size=4,\n",
    "                                    stride=2,\n",
    "                                    padding=1,\n",
    "                                    bias=False)),\n",
    "                         ('norm1', nn.BatchNorm2d(num_init_features)),\n",
    "                         ('relu1', nn.ReLU(inplace=True))]\n",
    "        if not no_max_pool:\n",
    "            self.input_features.append(\n",
    "                ('pool1', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)))\n",
    "        self.input_features = nn.Sequential(OrderedDict(self.input_features))\n",
    "        self.features = nn.Sequential()\n",
    "        # Each denseblock\n",
    "        num_actvec = actvec_size\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers,\n",
    "                                num_caps=self.num_caps,\n",
    "                                num_input_features=num_actvec,\n",
    "                                bn_size=bn_size,\n",
    "                                growth_rate=growth_rate,\n",
    "                                drop_rate=drop_rate,\n",
    "                                actvec_size=actvec_size)\n",
    "            self.features.add_module('denseblock{}'.format(i + 1), block)\n",
    "            num_actvec = num_actvec + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_caps = self.num_caps, \n",
    "                                    in_vect=actvec_size, \n",
    "                                    out_vect=actvec_size)\n",
    "                self.features.add_module('transition{}'.format(i + 1), trans)\n",
    "                num_actvec = num_actvec // 2\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "        self.metadata_network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(17, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(64, 128)\n",
    "        )\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(64 + 128, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, metadata):\n",
    "        input_features = self.input_features(x)\n",
    "        input_features = input_features.view(input_features.shape[0], int(self.num_init_features/self.actvec_size), self.actvec_size, input_features.shape[-2], input_features.shape[-1])\n",
    "\n",
    "        \n",
    "        \n",
    "        out = self.features(input_features)\n",
    "        y = self.metadata_network(metadata)\n",
    "        out = metadata[:, 0].unsqueeze(-1) + self.classifier(torch.cat((out.view(out.size(0), -1), y), dim=1))\n",
    "        return out\n",
    "    \n",
    "#     def forward(self, x, metadata):\n",
    "#         out = self.convolutions(x)\n",
    "#         out = out.view(x.size(0), -1)\n",
    "#         out_metadata = self.metadata_network(metadata)\n",
    "#         out = metadata[:, 0] + self.fc(torch.cat((out, out_metadata), dim=1))\n",
    "\n",
    "# #         out = ((torch.sigmoid(self.fc(torch.cat((out, out_metadata), dim=1))) * (self.range2 - self.range1)) + self.range1)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e0c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [121, 169, 201, 264]\n",
    "\n",
    "    if model_depth == 121:\n",
    "        model = DenseNetModel(num_init_features=64,\n",
    "                         growth_rate=4,\n",
    "                         block_config=(6, 12, 24, 16),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 169:\n",
    "        model = DenseNetModel(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 32, 32),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 201:\n",
    "        model = DenseNetModel(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 48, 32),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 264:\n",
    "        model = DenseNetModel(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 64, 48),\n",
    "                         **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24383d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.SubModel = generate_model(121)\n",
    "        \n",
    "#     seq_len, batch, input_size\n",
    "    def forward(self, x, y):\n",
    "        out = self.SubModel(x, y)\n",
    "        return out.flatten()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f071704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvapoDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.file_names = glob.glob(\"Train_Dataset/*\")\n",
    "        \n",
    "        self.vegs = [\n",
    "            \"WAT\",\n",
    "            \"ENF\",\n",
    "            \"EBF\",\n",
    "            \"DNF\",\n",
    "            \"DBF\",\n",
    "            \"MF\",\n",
    "            \"CSH\",\n",
    "            \"OSH\",\n",
    "            \"WSA\",\n",
    "            \"SAV\",\n",
    "            \"GRA\",\n",
    "            \"WET\",\n",
    "            \"CRO\",\n",
    "            \"URB\",\n",
    "            \"CVM\",\n",
    "            \"SNO\",\n",
    "            \"BSV\",\n",
    "            \"Missing Data\" \n",
    "        ]\n",
    "        self.clims = ['DFB', 'BWK', 'CFA', 'CWA', 'DWB', 'DFC', 'DFA', 'BSK', 'CSA', 'BSH']\n",
    "        \n",
    "        print(\"Dataset Length \" + str(len(self.file_names)))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        lon, lat, elev, veg, clim, geohash, year, month, day, cloud_coverage, pixel_coverage, true_et, pred_et = self.file_names[idx].split(\"_\")[-13:]\n",
    "        img = torch.from_numpy(np.load(self.file_names[idx]).astype(float))\n",
    "        et = float(self.file_names[idx].split(\"_\")[-1].replace(\".npy\", \"\"))\n",
    "        openet = float(self.file_names[idx].split(\"/\")[1].split(\"_\")[0])\n",
    "        date = \"_\".join(self.file_names[idx].split(\"_\")[-7:-4])\n",
    "        lat = float(self.file_names[idx].split(\"_\")[-12])\n",
    "        lon = float(self.file_names[idx].split(\"_\")[-13])\n",
    "        elev = np.array([float(self.file_names[idx].split(\"_\")[-11])/8848.0])\n",
    "        veg = torch.nn.functional.one_hot(torch.tensor(self.vegs.index(self.file_names[idx].split(\"_\")[-10].upper())), num_classes=len(self.vegs))\n",
    "        clim = torch.nn.functional.one_hot(torch.tensor(self.clims.index(self.file_names[idx].split(\"_\")[-9].upper())), num_classes=len(self.clims))\n",
    "        year = self.file_names[idx].split(\"_\")[-7]\n",
    "        month = self.file_names[idx].split(\"_\")[-6]\n",
    "        day = self.file_names[idx].split(\"_\")[-5]\n",
    "        \n",
    "        date_time_obj = datetime.strptime(year + '_' + month + '_' + day, '%Y_%m_%d')\n",
    "        day_of_year = date_time_obj.timetuple().tm_yday\n",
    "        day_sin = torch.tensor([np.sin(2 * np.pi * day_of_year/364.0)])\n",
    "        day_cos = torch.tensor([np.cos(2 * np.pi * day_of_year/364.0)])\n",
    "        \n",
    "        x_coord = torch.tensor([np.sin(math.pi/2-np.deg2rad(lat)) * np.cos(np.deg2rad(lon))])\n",
    "        y_coord = torch.tensor([np.sin(math.pi/2-np.deg2rad(lat)) * np.sin(np.deg2rad(lon))])\n",
    "        z_coord = torch.tensor([np.cos(math.pi/2-np.deg2rad(lat))])\n",
    "        \n",
    "        img = interpolate(img , size=32)[0]\n",
    "        \n",
    "        if img[20].mean() < 0:\n",
    "            lon_img = img[20].clone()\n",
    "            lat_img = img[19].clone()\n",
    "        else:\n",
    "            lat_img = img[20].clone()\n",
    "            lon_img = img[19].clone()\n",
    "            \n",
    "        img[19] = lon_img\n",
    "        img[20] = lat_img\n",
    "        \n",
    "        \n",
    "        img[[1,2,3,4]] = ((img[[1,2,3,4]] *0.0000275)-0.2)\n",
    "        img[7] = ((img[7] * 0.00341802) + 149.0)/400.0\n",
    "        \n",
    "        #Blue\n",
    "        #Green\n",
    "        #Red\n",
    "        #NIR\n",
    "        #LST\n",
    "        #Lon\n",
    "        #Lat\n",
    "        \n",
    "        output_img = img[[1,2,3,4,7,19,20]]\n",
    "        \n",
    "        rotations = random.randint(0, 3)\n",
    "        if (rotations == 1):\n",
    "            output_img = torch.rot90(output_img, 1, [1, 2])\n",
    "        elif (rotations == 2):\n",
    "            output_img = torch.rot90(output_img, 2, [1, 2])\n",
    "        elif (rotations == 3):\n",
    "            output_img = torch.rot90(output_img, 3, [1, 2])\n",
    "                                     \n",
    "        flip1 = random.randint(0, 1)\n",
    "        if (flip1 == 1):\n",
    "            output_img = torch.flip(output_img, (1,))\n",
    "        \n",
    "        flip2 = random.randint(0, 1)\n",
    "        if (flip2 == 1):\n",
    "            output_img = torch.flip(output_img, (2,))\n",
    "        \n",
    "        \n",
    "\n",
    "        return output_img, et, veg, clim, day_sin, day_cos, x_coord, y_coord, z_coord, elev, date, lon, lat, self.file_names[idx], openet         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EvapoDataset()\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=935, \n",
    "                                           num_workers=4, shuffle=True,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1197a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSEBop(torch.nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()    \n",
    "        self.lc = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n",
    "        self.scale = 1000\n",
    "        self.device = device\n",
    "        self.dict = {}\n",
    "        \n",
    "    \n",
    "    def _openet(self, date, lon, lat):\n",
    "        date_1 = [datetime.strptime(d1, \"%Y-%m-%d\") for d1 in date]\n",
    "        date_next_day = [str(d2 + timedelta(days=1))[:10] for d2 in date_1]\n",
    "        ets = []\n",
    "        \n",
    "        for i1 in range(len(date)):\n",
    "            \n",
    "            if str(date[i1]) + \"_\" + str(lon[i1]) + \"_\" + str(lat[i1]) in self.dict:\n",
    "                ets.append(self.dict[str(date[i1]) + \"_\" + str(lon[i1]) + \"_\" + str(lat[i1])])\n",
    "            else:\n",
    "                lc = self.lc.filterDate(date[i1], date_next_day[i1])\n",
    "                site1point = ee.Geometry.Rectangle(lon[i1].item(), lat[i1].item(), lon[i1].item(), lat[i1].item())\n",
    "                lc_r_poi = lc.getRegion(site1point, self.scale).getInfo()\n",
    "                id_list = [x[0] for x in lc_r_poi[1:]]\n",
    "                id_list.sort()\n",
    "                print(\"THISONE\", id_list)\n",
    "                print(\"date\", date[i1])\n",
    "                print(\"lon\", lon[i1])\n",
    "                print(\"lat\", lat[i1])\n",
    "                landsat_img = ee.Image('LANDSAT/LC08/C02/T1_L2/' + id_list[-1])\n",
    "                \n",
    "#                 landsat_region = site1point.buffer(225)\n",
    "#                 model_obj = model.Image.from_landsat_c2_sr(\n",
    "#                     landsat_img.clip(site1point.buffer(point_size)), \n",
    "#                     tcorr_source='FANO',\n",
    "#                     et_reference_source='IDAHO_EPSCOR/GRIDMET', \n",
    "#                     et_reference_band='etr', \n",
    "#                     et_reference_factor=0.85,\n",
    "#                     et_reference_resample='nearest',\n",
    "#                 )\n",
    "#                 et = model_obj.et.reduceRegion(ee.Reducer.max(), landsat_region).getInfo()[\"et\"]\n",
    "#                 if et:\n",
    "#                     ets.append(et)\n",
    "#                     self.dict[str(date[i1]) + \"_\" + str(lon[i1]) + \"_\" + str(lat[i1])] = et\n",
    "#                 else:\n",
    "#                     assert 0 == 1\n",
    "#                     ets.append(0.0)\n",
    "#                     self.dict[str(date[i1]) + \"_\" + str(lon[i1]) + \"_\" + str(lat[i1])] = 0.0\n",
    "                \n",
    "                \n",
    "                point_size = 225\n",
    "                while True:\n",
    "                    try:\n",
    "                        landsat_region = site1point.buffer(225)\n",
    "                        model_obj = model.Image.from_landsat_c2_sr(\n",
    "                            landsat_img.clip(site1point.buffer(point_size)), \n",
    "                            tcorr_source='FANO',\n",
    "                            et_reference_source='IDAHO_EPSCOR/GRIDMET', \n",
    "                            et_reference_band='etr', \n",
    "                            et_reference_factor=0.85,\n",
    "                            et_reference_resample='nearest',\n",
    "                        )\n",
    "        #                 print(\"HERE1\", model_obj.et.getInfo())\n",
    "                        print(\"HERE2\", model_obj.et.reduceRegion(ee.Reducer.max(), landsat_region).getInfo())\n",
    "                        et = model_obj.et.reduceRegion(ee.Reducer.max(), landsat_region).getInfo()[\"et\"]\n",
    "                        if et:\n",
    "                            ets.append(et)\n",
    "                            self.dict[str(date[i1]) + \"_\" + str(lon[i1]) + \"_\" + str(lat[i1])] = et\n",
    "                            break\n",
    "                        else:\n",
    "                            point_size = point_size + 225\n",
    "                            print(\"grow \" + str(point_size))\n",
    "                            continue\n",
    "    #                             ets.append(0.0)\n",
    "    #                             self.dict[str(date[i1]) + \"_\" + str(lon[i1]) + \"_\" + str(lat[i1])] = 0.0\n",
    "                    except:\n",
    "                        point_size = point_size + 225\n",
    "                        print(\"grow \" + str(point_size))\n",
    "        \n",
    "        return ets\n",
    "    \n",
    "    def forward(self, date, lat, lon):\n",
    "        #nir red lst\n",
    "        # etr, elev, sph, srad, tmin, tmax, lat, doy\n",
    "        \n",
    "#         data = data\n",
    "        date_str = [dstr.replace(\"_\", \"-\") for dstr in date]\n",
    "        \n",
    "        return torch.tensor(self._openet(date_str, lon, lat), device=self.device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561965da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_quench(epoch, model, loader):\n",
    "    with torch.no_grad():\n",
    "        et_correct = 0\n",
    "        counter = 0\n",
    "        for img_seq, et, veg, clim, day_sin, day_cos, x_coord, y_coord, z_coord, elev, dat, lon, lat, _, openet in loader:\n",
    "            img_seq = img_seq.to(device=device, dtype=torch.float32)\n",
    "            et = et.to(device=device, dtype=torch.float32)\n",
    "            veg = veg.to(device=device, dtype=torch.float32)\n",
    "            clim = clim.to(device=device, dtype=torch.float32)\n",
    "            day_sin = day_sin.to(device=device, dtype=torch.float32)\n",
    "            day_cos = day_cos.to(device=device, dtype=torch.float32)\n",
    "            x_coord = x_coord.to(device=device, dtype=torch.float32)\n",
    "            y_coord = y_coord.to(device=device, dtype=torch.float32)\n",
    "            z_coord = z_coord.to(device=device, dtype=torch.float32)\n",
    "            elev = elev.to(device=device, dtype=torch.float32)\n",
    "            lat = lat.to(device=device, dtype=torch.float32)\n",
    "            lon = lon.to(device=device, dtype=torch.float32)\n",
    "            openet = openet.to(device=device, dtype=torch.float32)\n",
    "        \n",
    "#             ssebop_ET = ssebop(dat, lat, lon)\n",
    "            ssebop_ET = openet.reshape(openet.shape[0], -1)\n",
    "        \n",
    "            output = model(img_seq[:, 0:5], torch.cat((ssebop_ET, clim, day_sin, day_cos, x_coord, y_coord, z_coord, elev), dim=1))\n",
    "        \n",
    "            et_correct += (torch.sum(torch.abs((output-et))))\n",
    "            counter += output.shape[0]\n",
    "\n",
    "        \n",
    "        return str(round(float(et_correct.sum() / counter), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7289d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainResAttnCap():\n",
    "\n",
    "    def __init__(self, level=1, epochs=500, batch_size=512, torch_type=torch.float32):\n",
    "        super(TrainResAttnCap, self).__init__()\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = \"cpu\"\n",
    "        if torch.cuda.is_available(): self.device = \"cuda\"\n",
    "        self.torch_type = torch_type\n",
    "        self.model_name = \"ResAttnCap\"\n",
    "        \n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        self.model = Model().to(self.device, dtype=torch.float32)\n",
    "        \n",
    "        self.train_dataset = EvapoDataset()\n",
    "        \n",
    "        self.dataset_size = len(self.train_dataset)\n",
    "        self.indices = list(range(self.dataset_size))\n",
    "        \n",
    "        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        \n",
    "        self.test_loader = torch.utils.data.DataLoader(self.train_dataset,batch_size=self.batch_size, num_workers=2, pin_memory=True)\n",
    "        \n",
    "        self.opt = torch.optim.Adagrad(self.model.parameters(), lr=0.01)\n",
    "        self.sched = torch.optim.lr_scheduler.StepLR(self.opt, step_size=10, gamma=0.999)\n",
    "#         self.ssebop_model = SSEBop(device).to(device)\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(1, 502):\n",
    "            start_time = time.time()\n",
    "            for ind, (img_seq, et, veg, clim, day_sin, day_cos, x_coord, y_coord, z_coord, elev, dat, lon, lat, _, openet) in enumerate(self.train_loader):\n",
    "                img_seq = img_seq.to(device=self.device, dtype=torch.float32)\n",
    "                true_et = et.to(device=self.device, dtype=torch.float32)\n",
    "                clim = clim.to(device=self.device, dtype=torch.float32)\n",
    "                day_sin = day_sin.to(device=self.device, dtype=torch.float32)\n",
    "                day_cos = day_cos.to(device=self.device, dtype=torch.float32)\n",
    "                x_coord = x_coord.to(device=self.device, dtype=torch.float32)\n",
    "                y_coord = y_coord.to(device=self.device, dtype=torch.float32)\n",
    "                z_coord = z_coord.to(device=self.device, dtype=torch.float32)\n",
    "                elev = elev.to(device=self.device, dtype=torch.float32)\n",
    "                lat = lat.to(device=self.device, dtype=torch.float32)\n",
    "                lon = lon.to(device=self.device, dtype=torch.float32)\n",
    "                openet = openet.to(device=device, dtype=torch.float32)\n",
    "                self.opt.zero_grad()\n",
    "                ssebop_ET = openet\n",
    "                ssebop_ET = ssebop_ET.reshape(ssebop_ET.shape[0], -1)\n",
    "                output = self.model(img_seq[:, 0:5], torch.cat((ssebop_ET, clim, day_sin, day_cos, x_coord, y_coord, z_coord, elev), dim=1))\n",
    "                loss = self.mse(output, true_et)\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "#                 print(\"===> \" + str(ind + 1) + \"/\" + str(int(self.dataset_size/self.batch_size)) + \", \" + str(loss))\n",
    "            self.sched.step()\n",
    "            test_accuracy = test_quench(epoch, self.model, self.test_loader) \n",
    "            print(\"Epoch \" + str(epoch) + \", Test \" + test_accuracy )\n",
    "            if epoch % 50 == 0.0:\n",
    "                torch.save(self.model, \"Checkpoints/Quench_\" + str(epoch) + \".pt\" )\n",
    "                print(\"Saved \" + str(epoch) + \" Epoch Model\")\n",
    "             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb585561",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = TrainResAttnCap()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6693a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
